{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":29907,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementation of GAN for beginners.\n\n**generative adversarial networks (GANs)** are able to learn how to reproduce synthetic data that looks real. GANs is ***the most interesting idea in the last 10 years in ML,*** by Yann LeCun, one of the fathers of deep learning.\nIn this notebook, we will going to train a GAN to reproduce our dataset.\nTo facilitate understanding, i chose to do simple with the dataset. So i choose to create a dataset by generating random numbers following normal law. Thus, we can change the parameter(`maen` and `stddev`) of this low to observe the impact on the GAN learning and for more fun.\nGAN is not only for image data. We will going to train GAN on simple numerical one dimension data.\n\nFor more understanding of GAN concept, please visit this notebook https://www.kaggle.com/roblexnana/understanding-generative-models-gan\n\n### Table of interest:\n1. Create our dataset.\n2. Define our generator and our disciminator.\n3. Define our losses functions.\n4. Define the optimizers and the train operations.\n5. Training and visualization of results.\n\nLet's get started.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras import Model\n\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Create our dataset.","metadata":{}},{"cell_type":"code","source":"# fonction to generate our dataset\ndef sample_dataset():\n    dataset_shape = (2000, 1)\n    return tf.random.normal(mean=8., shape=dataset_shape, stddev=0.5, dtype=tf.float32)\n\n# visualize our data with histogram\nplt.hist(sample_dataset().numpy(), 100)\naxes = plt.gca()\naxes.set_xlim([-1,11])\naxes.set_ylim([0, 70])\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Define our generator and our disciminator.","metadata":{}},{"cell_type":"code","source":"def generator(input_shape):\n    \"\"\"Defines the generator keras.Model.\n    Args:\n        input_shape: the desired input shape (e.g.: (latent_space_size))\n    Returns:\n        G: The generator model\n    \"\"\"\n    inputs = Input(input_shape)\n    net = Dense(units=64, activation=tf.nn.elu, name=\"fc1\")(inputs)\n    net = Dense(units=64, activation=tf.nn.elu, name=\"fc2\")(net)\n    net = Dense(units=1, name=\"G\")(net)\n    G = Model(inputs=inputs, outputs=net)\n    return G\n\ndef disciminator(input_shape):\n    \"\"\"Defines the Discriminator keras.Model.\n    Args:\n        input_shape: the desired input shape (e.g.: (the generator output shape))\n    Returns:\n        D: the Discriminator model\n    \"\"\"\n    inputs = Input(input_shape)\n    net = Dense(units=32, activation=tf.nn.elu, name=\"fc1\")(inputs)\n    net = Dense(units=1, name=\"D\")(net)\n    D = Model(inputs=inputs, outputs=net)\n    return D\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the real input shape\ninput_shape = (1,)\n\n# Define the Discriminator model\nD = disciminator(input_shape)\n\n# Arbitrary set the shape of the noise prior\nlatent_space_shape = (100,)\n# Define the input noise shape and define the generator\nG = generator(latent_space_shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Define our losses functions","metadata":{}},{"cell_type":"code","source":"# Define the losses fonctions to optimize\nbce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n# Distriminator loss foction\ndef d_loss(d_real, d_fake):\n    \"\"\"The disciminator loss function.\"\"\"\n    return bce(tf.ones_like(d_real), d_real) + bce(tf.zeros_like(d_fake), d_fake)\n# Generator loss fonction\ndef g_loss(generated_output):\n    \"\"\"The Generator loss function.\"\"\"\n    return bce(tf.ones_like(generated_output), generated_output)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Define the optimizers and the train operations.","metadata":{}},{"cell_type":"code","source":"\n# Define the optimizers and the train operations\noptimizer = tf.keras.optimizers.Adam(1e-5)\n\n@tf.function\ndef train_step():\n    with tf.GradientTape(persistent=True) as tape:\n        real_data = sample_dataset()\n        noise_vector = tf.random.normal(\n            mean=0, stddev=1,\n            shape=(real_data.shape[0], latent_space_shape[0]))\n        # Sample from the Generator\n        fake_data = G(noise_vector)\n        # Compute the D loss\n        d_fake_data = D(fake_data)\n        d_real_data = D(real_data)\n        d_loss_value = d_loss(d_real_data, d_fake_data)\n        # Compute the G loss\n        g_loss_value = g_loss(d_fake_data)\n    # Now that we comptuted the losses we can compute the gradient\n    # and optimize the networks\n    d_gradients = tape.gradient(d_loss_value, D.trainable_variables)\n    g_gradients = tape.gradient(g_loss_value, G.trainable_variables)\n    # Deletng the tape, since we defined it as persistent\n    # (because we used it twice)\n    del tape \n    optimizer.apply_gradients(zip(d_gradients, D.trainable_variables))\n    optimizer.apply_gradients(zip(g_gradients, G.trainable_variables))\n    return real_data, fake_data, g_loss_value, d_loss_value\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Training and visualization of results.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfor step in range(30000):\n    real_data, fake_data, g_loss_value, d_loss_value = train_step()\n    if step % 2000 == 0:\n        print(\"G loss: \", g_loss_value.numpy(), \" D loss: \", d_loss_value.numpy(), \" step: \", step)\n\n        # Sample 5000 values from the Generator and draw the histogram\n        ax.hist(fake_data.numpy(), 100)\n        ax.hist(real_data.numpy(), 100)\n        # these are matplotlib.patch.Patch properties\n        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n\n        # place a text box in upper left in axes coords\n        textstr = f\"step={step}\"\n        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n                verticalalignment='top', bbox=props)\n\n        axes = plt.gca()\n        axes.set_xlim([-1,11])\n        axes.set_ylim([0, 60])\n        display(plt.gcf())\n        plt.gca().clear() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Hope that you find this notebook helpful. More to come.**\n\n**Please upvote this, to keep me motivate for doing better.**\n\n**Thanks.**","metadata":{}}]}